{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Co-training_3C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHX+VV6lfQnmsvCq6PQHEr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssvadla/Sharing/blob/main/Co_training_3C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2l8coN-MtHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7546e00f-7c9c-4fe9-d060-476e62b89bc5"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "import math\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.utils import resample\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import lightgbm as lgb\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXWl6z0dO-u4"
      },
      "source": [
        "record_unlabel_list = []\n",
        "record_f1_score = []\n",
        "record_CR = []"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkbUcWRPHvjJ"
      },
      "source": [
        "def training(unlabel_1,train,i,j):\n",
        "    train_size = len(train)\n",
        "    record_unlabel_list.append(train_size - 4416)\n",
        "    target_values = np.unique(train['Target'].values)\n",
        "\n",
        "    data_list = []\n",
        "    data_length_list = []\n",
        "    for i in target_values:\n",
        "      df_k = train[train['Target']==i]\n",
        "      data_list.append(df_k)\n",
        "      data_length_list.append(len(df_k))\n",
        "   \n",
        "\n",
        "    maximum_data = max(data_length_list)\n",
        "    print(\"maximum_data\",maximum_data)\n",
        "\n",
        "\n",
        "    ratio = math.floor(( 4 / 6 )* maximum_data)\n",
        "    print(\"ratio\",ratio)\n",
        "    loop_count = 0\n",
        "    train_upsampled = []\n",
        "    for i in data_length_list:\n",
        "      #print(i)\n",
        "      #print(len(data_list[loop_count]))\n",
        "      if i < ratio:\n",
        "        df_upsampled = resample(data_list[loop_count],replace=True,n_samples=ratio,random_state=123)\n",
        "      else:\n",
        "        df_upsampled = resample(data_list[loop_count],replace=True,n_samples=i,random_state=123)\n",
        "      print(\"@\",len(df_upsampled))\n",
        "      train_upsampled.append(df_upsampled)\n",
        "      loop_count = loop_count + 1\n",
        "\n",
        "    df_res = pd.concat(train_upsampled)\n",
        "    train = df_res.copy(deep = True)\n",
        "    print(\"len of train $$$\",len(train))\n",
        "\n",
        "    i_list.append(i)\n",
        "    j_list.append(j)\n",
        "    print(\"length of the UL received\",len(unlabel_1))\n",
        "    \n",
        "    #cleaning\n",
        "\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "    stopword=nltk.corpus.stopwords.words('english')\n",
        "\n",
        "    wl= WordNetLemmatizer()\n",
        "\n",
        "    def clean_text(text):\n",
        "      text=\"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "      tokens = re.split('\\W+',text)\n",
        "      text = [wl.lemmatize(word) for word in tokens if word not in stopword]\n",
        "      return text\n",
        "\n",
        "\n",
        "\n",
        "    tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
        "    #X_tfidf = tfidf_vect.fit_transform(train['Sentence'])\n",
        "    X_tfidf = tfidf_vect.fit_transform(train['text'])\n",
        "    print(X_tfidf.shape)\n",
        "\n",
        "    test = pd.read_csv(r'/content/drive/My Drive/Research/test_data.csv')\n",
        "\n",
        "    test['Target']=test['Target'].replace(['Others'],'Invalid')\n",
        "    test['Sentence'] = test['Sentence'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
        "    test['Sentence'] = test['Sentence'].str.replace('[^\\w\\s]','')\n",
        "    from nltk.corpus import stopwords\n",
        "    words = stopwords.words('english')\n",
        "    test['Sentence'] = test['Sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in words))\n",
        "    t_p = tfidf_vect.transform(test['Sentence'])\n",
        "\n",
        "\n",
        "\n",
        "    classifier_rf = RandomForestClassifier(n_estimators = 350, criterion = 'gini', max_features = 'auto', random_state = 42)\n",
        "    classifier_lgb = lgb.LGBMClassifier()\n",
        "    classifier_svm = svm.LinearSVC(multi_class='ovr',class_weight='balanced')\n",
        "\n",
        "    #train = train.rename(columns={'Sentence':'text'})\n",
        "    train.head()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #X, y = make_classification(random_state=1)\n",
        "\n",
        "    X_train, x_val, Y_train, y_val = train_test_split(X_tfidf,train['Target'],test_size=0.20,random_state=42)\n",
        "\n",
        "    X_train_whole = X_tfidf\n",
        "    Y_train_whole = train['Target']\n",
        "    #X_train_whole, x_val_whole, Y_train_whole, y_val_whole = train_test_split(X_tfidf,train['Target'],test_size=0.1,random_state=42)\n",
        "\n",
        "    \n",
        "    classifier_rf.fit(X_train, Y_train)\n",
        "    classifier_lgb.fit(X_train, Y_train)\n",
        "    classifier_svm.fit(X_train, Y_train)\n",
        "\n",
        "    val_pred_rf = classifier_rf.predict(x_val)\n",
        "    val_pred_lgb = classifier_lgb.predict(x_val)\n",
        "    val_pred_svm = classifier_svm.predict(x_val)\n",
        "\n",
        "    Accuracy_score_rf = accuracy_score(y_val,val_pred_rf)\n",
        "    print(\"Accuracy_score_rf \",Accuracy_score_rf)\n",
        "    Accuracy_score_lgb = accuracy_score(y_val,val_pred_lgb)\n",
        "    print(\"Accuracy_score_lgb \",Accuracy_score_lgb)\n",
        "    Accuracy_score_svm = accuracy_score(y_val,val_pred_svm)\n",
        "    print(\"Accuracy_score_svm \",Accuracy_score_svm)\n",
        "\n",
        "\n",
        "    classification_report_test_rf = classification_report(y_val,val_pred_rf)\n",
        "    print(\"classification report rf\",classification_report_test_rf)\n",
        "    classification_report_test_lgb = classification_report(y_val,val_pred_lgb)\n",
        "    print(\"classification report lgb\",classification_report_test_lgb)\n",
        "    classification_report_test_svm = classification_report(y_val,val_pred_svm)\n",
        "    print(\"classification report svm \",classification_report_test_svm)\n",
        "    \n",
        "    cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "    cross_valid_rf = cross_val_score(classifier_rf,x_val ,y_val, scoring = 'f1_weighted', cv=cv, n_jobs=-1).mean()\n",
        "    print(\"cross validation score RF\",cross_valid_rf)\n",
        "    cross_valid_lgb = cross_val_score(classifier_lgb,x_val ,y_val, scoring = 'f1_weighted', cv=cv, n_jobs=-1).mean()\n",
        "    print(\"cross validation score LGB\",cross_valid_lgb)\n",
        "    cross_valid_svm = cross_val_score(classifier_svm,x_val ,y_val, scoring = 'f1_weighted', cv=cv, n_jobs=-1).mean()\n",
        "    print(\"cross validation score SVM\",cross_valid_svm)\n",
        "\n",
        "\n",
        "    f1_score_rf = f1_score(y_val,val_pred_rf,average='weighted')\n",
        "    print(\" f1 score test rf \",f1_score_rf )\n",
        "    f1_score_lgb = f1_score(y_val,val_pred_lgb, average='weighted')\n",
        "    print(\" f1 score test lgb \",f1_score_lgb )\n",
        "    f1_score_svm = f1_score(y_val,val_pred_svm, average='weighted')\n",
        "    print(\" f1 score test svm \",f1_score_svm )\n",
        "\n",
        "    if (Accuracy_score_rf > Accuracy_score_lgb) and (Accuracy_score_rf > Accuracy_score_svm):\n",
        "      B = classifier_rf\n",
        "    elif (Accuracy_score_svm > Accuracy_score_lgb) and (Accuracy_score_svm > Accuracy_score_rf):\n",
        "      B = classifier_svm\n",
        "    else:\n",
        "      B = classifier_lgb\n",
        "\n",
        "\n",
        "    B.fit(X_train_whole,Y_train_whole)\n",
        "    print(\"len of train tfid #####\",len(Y_train_whole))\n",
        "    test_pred = B.predict(t_p)\n",
        "    accuracy_test = accuracy_score(test['Target'],test_pred)\n",
        "    print('Accuracy test data', accuracy_test)\n",
        "    f1_score_test = f1_score(test['Target'],test_pred, average='weighted')\n",
        "    print(\" f1 score test \",f1_score_test )\n",
        "    \n",
        "    f1_score_test_list.append(f1_score_test)\n",
        "    record_f1_score.append(f1_score_test)\n",
        "    classification_report_test_data = classification_report(test['Target'],test_pred)\n",
        "    record_CR.append(classification_report_test_data)\n",
        "    print(\"classification report \",classification_report_test_data)\n",
        "    \n",
        "    # class_x_un1 = tfidf_vect.transform(unlabel_1['text'])\n",
        "    # pseudo_pred = B.predict(class_x_un1)\n",
        "\n",
        "    class_x_un1 = tfidf_vect.transform(unlabel_1['text'])\n",
        "\n",
        "    class_pred_unlabel_1 = B.predict(class_x_un1)\n",
        "    unlabel_1['Target']=class_pred_unlabel_1\n",
        "    frame_1 = [train,unlabel_1]\n",
        "    train_1 = pd.concat(frame_1)\n",
        "    \n",
        "    print(len(train_1))\n",
        "    \n",
        "    \n",
        "    \n",
        "    with open(r'/content/drive/My Drive/Results/Co_Training_3C/Result01.txt', 'a') as writefile:\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" Co-training results \")\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" train_size \")\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(str(train_size))\n",
        "        writefile.write(\" Accuracy_score_rf = \")\n",
        "        writefile.write(str(Accuracy_score_rf))\n",
        "        writefile.write(\" Accuracy_score_lgb = \")\n",
        "        writefile.write(str(Accuracy_score_lgb))\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" Accuracy_score_svm = \")\n",
        "        writefile.write(str(Accuracy_score_svm))\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" classification_report_test_rf = \")\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(str(classification_report_test_rf))\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" classification_report_test_lgb = \")\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(str(classification_report_test_lgb))\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" classification_report_test_svm = \")\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(str(classification_report_test_svm))\n",
        "        writefile.write(\"\\n\")\n",
        "      \n",
        "        writefile.write(\" cross_valid_rf = \")\n",
        "        writefile.write(str(cross_valid_rf))\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" cross_valid_lgb = \")\n",
        "        writefile.write(str(cross_valid_lgb))\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" cross_valid_svm = \")\n",
        "        writefile.write(str(cross_valid_svm))\n",
        "        writefile.write(\"\\n\")\n",
        "        \n",
        "        writefile.write(\" f1_score_rf = \")\n",
        "        writefile.write(str(f1_score_rf))\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" f1_score_lgb = \")\n",
        "        writefile.write(str(f1_score_lgb))\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" f1_score_svm = \")\n",
        "        writefile.write(str(f1_score_svm))\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" Results on test data \")\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" f1_score_test = \")\n",
        "        writefile.write(str(f1_score_test))\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" classifier selected = \")\n",
        "        writefile.write(str(B))\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" classification_report_test_data = \")\n",
        "        writefile.write(str(classification_report_test_data))\n",
        "        writefile.write(\"\\n\")\n",
        "    \n",
        "    \n",
        "    \n",
        "    return train_1,train_size_list,f1_score_test_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # print('Validation Accuracy_score: %f' % Accuracy_score)\n",
        "    # test_pred = classifier.predict(t_p)\n",
        "    # acc_test = accuracy_score(test_pred,test['Target'])\n",
        "    # print('Accuracy test data', acc_test)\n",
        "    # matrix = confusion_matrix(y_pred, y_val)\n",
        "    # print(matrix)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okqw6tYFH1Xx"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcHbFL_gH7Mr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7802338d-a837-4223-f117-159b22a66f6c"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopword=nltk.corpus.stopwords.words('english')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wl= WordNetLemmatizer()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwFto_a-H_ha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5260b108-4d8b-4f26-f9ed-ea497d29d761"
      },
      "source": [
        "unlabel = pd.read_csv(r'/content/drive/My Drive/Research/Unlabeled_data.csv')\n",
        "unlabel.head()\n",
        "\n",
        "\n",
        "del unlabel['Complete']\n",
        "\n",
        "del unlabel['Unnamed: 0']\n",
        "\n",
        "unlabel.head()\n",
        "\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
        "unlabel['text'] = unlabel['text'].str.replace('[^\\w\\s]','')\n",
        "from nltk.corpus import stopwords\n",
        "words = stopwords.words('english')\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in words))\n",
        "\n",
        "\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: TextBlob(x).words)\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x]))\n",
        "\n",
        "unlabel.head()\n",
        "\n",
        "\n",
        "len(unlabel)\n",
        "\n",
        "\n",
        "\n",
        "# unlabel_1 = unlabel.loc[:100]\n",
        "# unlabel_1.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "537703"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpNng9BAIC8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37009f69-c0c5-4e22-ea1c-b48e9235eef9"
      },
      "source": [
        "\n",
        "\n",
        "train1 = pd.read_csv('/content/drive/My Drive/Research/train_data1.csv')\n",
        "train2 = pd.read_csv('/content/drive/My Drive/Research/train_data2.csv')\n",
        "train3 = pd.read_csv('/content/drive/My Drive/Research/train_data3.csv')\n",
        "train4 = pd.read_csv('/content/drive/My Drive/Research/train_data4.csv')\n",
        "train5 = pd.read_csv('/content/drive/My Drive/Research/train_data5.csv')\n",
        "train6 = pd.read_csv('/content/drive/My Drive/Research/train_data6.csv')\n",
        "train7 = pd.read_csv('/content/drive/My Drive/Research/train_data7.csv')\n",
        "train8 = pd.read_csv('/content/drive/My Drive/Research/train_data8.csv')\n",
        "train9 = pd.read_csv('/content/drive/My Drive/Research/train_data9.csv')\n",
        "train10 = pd.read_csv('/content/drive/My Drive/Research/train_data10.csv')\n",
        "train_highKappa = pd.read_csv('/content/drive/My Drive/Research/train_data_highkappa.csv')\n",
        "train1.head()\n",
        "\n",
        "train1.head()\n",
        "train = train1\n",
        "train_list = [train2,train3,train4,train5,train6,train7,train8,train9,train10,train_highKappa]\n",
        "for i in train_list:\n",
        "    #print(i)\n",
        "    train = train.append(i)\n",
        "\n",
        "#unlabel_size = unlabel_size\n",
        "#Threshold=Threshold\n",
        "train.sort_values(\"Sentence\", inplace = True)\n",
        "print(len(train))\n",
        "\n",
        "train = train.drop_duplicates(subset =\"Sentence\")\n",
        "\n",
        "train['Target'].unique()\n",
        "\n",
        "\n",
        "train['Target']=train['Target'].replace(['Others'],'Invalid')\n",
        "train['Target'].unique()\n",
        "print(len(train))\n",
        "train = train.rename(columns={'Sentence':'text'})\n",
        "all_Data_train_size = len(train)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37711\n",
            "4416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh5gDu9WAfj-"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuZ_VgwFCm6y",
        "outputId": "553d0392-b168-45ac-9560-81cd6369cb67"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4916"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sVdhkwpIGS6"
      },
      "source": [
        "#length_list = range(1000,100000,5000)\n",
        "length_list = [500]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NUH2vopILJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8f3f586-1953-43e1-9c40-048695e442c9"
      },
      "source": [
        "j=0\n",
        "iteration = 0\n",
        "i_list = []\n",
        "j_list = []\n",
        "f1_score_test_list = []\n",
        "train_size_list =[]\n",
        "\n",
        "for i in length_list:\n",
        "  print(\"iteration \",iteration)\n",
        "  print(i)\n",
        "  print(j)\n",
        "  unlabel_chunk = unlabel[j:i]\n",
        "  train,op_train_size_list,op_f1_score_test_list = training(unlabel_chunk,train,i,j)\n",
        "  j=i\n",
        "  iteration = iteration + 1\n",
        "  print(\"length\",len(unlabel_chunk))\n",
        "    \n",
        "with open(r'/content/drive/My Drive/Results/Co_Training_3C/Result01.txt', 'a') as writefile:\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\" Co-training results \")\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\" train size \")\n",
        "    writefile.write(str(all_Data_train_size))\n",
        "    writefile.write(\"\\n\")\n",
        "#     writefile.write(\"op_unlabel_size\")\n",
        "#     writefile.write(\"\\n\")\n",
        "#     writefile.write(str(op_unlabel_size))\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\"op_train_size_list\")\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(str(op_train_size_list))\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\"op_f1_score_test_list\")\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(str(op_f1_score_test_list))\n",
        "    \n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration  0\n",
            "500\n",
            "0\n",
            "maximum_data 2253\n",
            "ratio 1502\n",
            "@ 1502\n",
            "@ 1502\n",
            "@ 2253\n",
            "@ 1502\n",
            "@ 1502\n",
            "@ 1502\n",
            "len of train $$$ 9763\n",
            "length of the UL received 500\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "(9763, 6515)\n",
            "Accuracy_score_rf  0.9482846902201741\n",
            "Accuracy_score_lgb  0.8965693804403482\n",
            "Accuracy_score_svm  0.9339477726574501\n",
            "classification report rf                   precision    recall  f1-score   support\n",
            "\n",
            "        Analysis       0.94      0.90      0.92       321\n",
            "      Conclusion       0.97      0.98      0.98       299\n",
            "           Facts       0.94      0.90      0.92       435\n",
            "         Invalid       0.89      0.96      0.92       294\n",
            "           Issue       0.98      1.00      0.99       295\n",
            "Rule/Law/Holding       0.98      0.97      0.98       309\n",
            "\n",
            "        accuracy                           0.95      1953\n",
            "       macro avg       0.95      0.95      0.95      1953\n",
            "    weighted avg       0.95      0.95      0.95      1953\n",
            "\n",
            "classification report lgb                   precision    recall  f1-score   support\n",
            "\n",
            "        Analysis       0.88      0.80      0.84       321\n",
            "      Conclusion       0.92      0.98      0.95       299\n",
            "           Facts       0.86      0.86      0.86       435\n",
            "         Invalid       0.84      0.86      0.85       294\n",
            "           Issue       0.95      0.97      0.96       295\n",
            "Rule/Law/Holding       0.94      0.94      0.94       309\n",
            "\n",
            "        accuracy                           0.90      1953\n",
            "       macro avg       0.90      0.90      0.90      1953\n",
            "    weighted avg       0.90      0.90      0.90      1953\n",
            "\n",
            "classification report svm                    precision    recall  f1-score   support\n",
            "\n",
            "        Analysis       0.91      0.91      0.91       321\n",
            "      Conclusion       0.92      0.97      0.95       299\n",
            "           Facts       0.95      0.87      0.91       435\n",
            "         Invalid       0.93      0.92      0.92       294\n",
            "           Issue       0.97      0.98      0.97       295\n",
            "Rule/Law/Holding       0.93      0.98      0.95       309\n",
            "\n",
            "        accuracy                           0.93      1953\n",
            "       macro avg       0.93      0.94      0.94      1953\n",
            "    weighted avg       0.93      0.93      0.93      1953\n",
            "\n",
            "cross validation score RF 0.7251713334393902\n",
            "cross validation score LGB 0.6585486884820628\n",
            "cross validation score SVM 0.7093221472103304\n",
            " f1 score test rf  0.9481118645910419\n",
            " f1 score test lgb  0.8958878995944538\n",
            " f1 score test svm  0.9335602180695292\n",
            "len of train tfid ##### 9763\n",
            "Accuracy test data 0.6353166986564299\n",
            " f1 score test  0.6287483079542432\n",
            "classification report                    precision    recall  f1-score   support\n",
            "\n",
            "        Analysis       0.48      0.31      0.38        77\n",
            "      Conclusion       0.70      0.73      0.72        26\n",
            "           Facts       0.72      0.78      0.75       267\n",
            "         Invalid       0.44      0.57      0.49        83\n",
            "           Issue       0.89      0.50      0.64        34\n",
            "Rule/Law/Holding       0.57      0.47      0.52        34\n",
            "\n",
            "        accuracy                           0.64       521\n",
            "       macro avg       0.63      0.56      0.58       521\n",
            "    weighted avg       0.64      0.64      0.63       521\n",
            "\n",
            "10263\n",
            "length 500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:162: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZKH4YPhAmfp"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF2FTyTcCXB3",
        "outputId": "c20c6322-de0b-4c0f-d165-37593b0ee8c7"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10263"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9REEAwnmJm3-",
        "outputId": "9d5f4ff9-11b5-4c8f-b57a-677c21933238",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10263"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whPxqUW2CTc7",
        "outputId": "a5c49f7a-cc34-4912-8f6c-eed6d1545436"
      },
      "source": [
        "len(train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10263"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n88BRkXPIOXm"
      },
      "source": [
        "with open(r'/content/drive/My Drive/Results/Co_Training_3C/Result01.txt', 'a') as writefile:\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\" Co-training results \")\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\" train size \")\n",
        "    writefile.write(str(all_Data_train_size))\n",
        "    writefile.write(\"\\n\")\n",
        "#     writefile.write(\"op_unlabel_size\")\n",
        "#     writefile.write(\"\\n\")\n",
        "#     writefile.write(str(op_unlabel_size))\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\"op_train_size_list\")\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(str(op_train_size_list))\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\"op_f1_score_test_list\")\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(str(op_f1_score_test_list))\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYqGGD-Dmvme"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbNboFOZASL3"
      },
      "source": [
        "with open(r'/content/drive/My Drive/Results/Co_Training_3C/Result02.txt', 'a') as writefile:\n",
        "\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(\"record_unlabel_list\")\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(str(record_unlabel_list))\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(\"record_f1_score\")\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(str(record_f1_score))\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(\"record_CR\")\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(str(record_CR))\n"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}